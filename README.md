# Bayesian Optimization in Junction Tree Variantional Autoencoder.

Implementation of [Junction Tree Variational Autoencoder for Molecular Graph Generation](https://arxiv.org/abs/1802.04364) latent space Bayesian optimization using [GPytorch](https://gpytorch.ai/) and [DGL](https://www.dgl.ai/).


## Required file:

* `jtnn/` contains JTNN model codes
* `vocab.txt` contains vocabulary list used for training JTNN
* `model.iter` contains your pretrained JTNN model
* `latent_features.txt` and  `targets.txt` contains the latent representations of your dataset and their corresponding target scores

## Files explained:

* `GPytorch_BO.py` is the main file performing Bayesian Optimization implemented using GPytorch, to run this code:
```
python GPytroch_BO.py -t 200000 -e 250 -v path_to_vocab -m path_to_model -l path_to_latent_features -s path_to_target_scores -o output_dir
```
the code will generate a list of valid molecules in a saved file.

## Demo:

In short, Junction Tree VAE is a generative model for molecule generation. It is an variantional autoencoder aiming to generate valid molecules by encoding both their molecule graph representations and functional group clustering tree representations. A succesfully-trained JT-VAE is able to encode a molecule into two multi-dimensional latent representations, one stores the information of the molecule graph, the other stores the information of the tree structure.

![](https://i.imgur.com/uflej98.png)

---

### 1. Objective Molecule Generation: Optimize Octanal Water-partition Coefficient (LogP):

We can model a desired function $\mathcal F$ in the latent space $\Omega$ that maps each molecule latent representation $x \in \Omega$ to a desired attributes $y \in \mathcal Y$ using Sparse Gaussian Process Regression. After training the SGP, we then perform a Bayesian Optimization on the function $\mathcal F$ to find the potential minimum of the function with a credible interval (confidence band in Bayesian nomenclature).

---

### 2. Latent Space Molecule Neighborhood Exploration:

The latent space generated by Junction Tree VAE is a multivariate Gaussian distribution. In our case, a 28-dimensional Gaussian with mean zero and variance one.\
We try to explore a 2-dimensional neighborhood of a given molecule by first generating two orthogonal Gaussian noises. Expand and contrast these two Gaussian noises to different scale and add to the latent representation to create a grid of molecule representations. Decode these representations to get the neighborhood of the target molecule in the latent space. \
We show some demos here:
![](https://i.imgur.com/FZndOqO.png)


![](https://i.imgur.com/WL6aCnM.png)

The center molecule in these $7 \times 7$ grid is the input target molecule.

![](https://i.imgur.com/fbvLE77.png)

This grid is generated by only adding noises to first and second dimension of the latent representation in different scale. We don't yet understand the meaning encoded in different dimension. This way of exploration gives us an approach to survey the information encoded in different dimension.

---
### 3. Sampling from Latent Space:

The latent space is designed to be a multivariate Gaussian with mean zero and variance one. So we perform a random sampling in this space to generate various molecules. 
![](https://i.imgur.com/qwTuTXF.png)

This is a demo of 49 molecules generated by JT-VAE. The sampling parameters are Normal(0, I) for tree latent representation and Normal(0, I) for graph representation.
![](https://i.imgur.com/tsLrYHH.png)

Normal(0, 2 * I) for tree latent representation and Normal(0, I) for graph representation.

![](https://i.imgur.com/6q1uimj.png)

Normal(0, I) for tree latent representation and Normal(0, 2 * I) for graph representation.
