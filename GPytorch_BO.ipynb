{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18000]) torch.Size([2000])\n",
      "Iter 1/20 - Loss: 2.646\n",
      "Iter 2/20 - Loss: 2.609\n",
      "Iter 3/20 - Loss: 2.574\n",
      "Iter 4/20 - Loss: 2.542\n",
      "Iter 5/20 - Loss: 2.512\n",
      "Iter 6/20 - Loss: 2.483\n",
      "Iter 7/20 - Loss: 2.457\n",
      "Iter 8/20 - Loss: 2.433\n",
      "Iter 9/20 - Loss: 2.410\n",
      "Iter 10/20 - Loss: 2.390\n",
      "Iter 11/20 - Loss: 2.371\n",
      "Iter 12/20 - Loss: 2.353\n",
      "Iter 13/20 - Loss: 2.336\n",
      "Iter 14/20 - Loss: 2.321\n",
      "Iter 15/20 - Loss: 2.308\n",
      "Iter 16/20 - Loss: 2.295\n",
      "Iter 17/20 - Loss: 2.283\n",
      "Iter 18/20 - Loss: 2.273\n",
      "Iter 19/20 - Loss: 2.263\n",
      "Iter 20/20 - Loss: 2.254\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "import scipy.stats as sps\n",
    "import numpy as np\n",
    "import os.path\n",
    "import time\n",
    "\n",
    "import dgl\n",
    "from dgl.data.utils import download, extract_archive, get_download_dir\n",
    "\n",
    "import rdkit\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.Chem import MolFromSmiles, MolToSmiles\n",
    "from rdkit.Chem import rdmolops\n",
    "from bo import sascorer\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "from torch.distributions import constraints, transform_to\n",
    "\n",
    "from jtnn import *\n",
    "\n",
    "\n",
    "# from optparse import OptionParser\n",
    "import argparse\n",
    "\n",
    "torch.backends.cudnn.enabled=True\n",
    "\n",
    "lg = rdkit.RDLogger.logger()\n",
    "lg.setLevel(rdkit.RDLogger.CRITICAL)\n",
    "\n",
    "\n",
    "# We define the functions used to load and save objects\n",
    "def save_object(obj, filename):\n",
    "    result = pickle.dumps(obj)\n",
    "    with gzip.GzipFile(filename, 'wb') as dest: dest.write(result)\n",
    "    dest.close()\n",
    "\n",
    "\n",
    "def load_object(filename):\n",
    "    with gzip.GzipFile(filename, 'rb') as source: result = source.read()\n",
    "    ret = pickle.loads(result)\n",
    "    source.close()\n",
    "    return ret\n",
    "\n",
    "\n",
    "# parser = OptionParser()\n",
    "\"\"\"\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-v\", \"--vocab\", dest=\"vocab_path\", default=\"/home/ubuntu/ASAIL/jtnn_bo/jtnn/vocab.txt\")\n",
    "parser.add_argument(\"-m\", \"--model\", dest=\"model_path\", required=True)\n",
    "parser.add_argument(\"-o\", \"--save_dir\", dest=\"save_dir\", required=True)\n",
    "parser.add_argument(\"-n\", \"--n_train\", dest=\"training_num\", default=500)\n",
    "parser.add_argument(\"-i\", \"--n_ind\", dest=\"inducing_num\", default=500)\n",
    "parser.add_argument(\"-w\", \"--hidden\", dest=\"hidden_size\", default=200)\n",
    "parser.add_argument(\"-l\", \"--latent\", dest=\"latent_size\", default=56)\n",
    "parser.add_argument(\"-d\", \"--depth\", dest=\"depth\", default=3)\n",
    "parser.add_argument(\"-r\", \"--seed\", dest=\"random_seed\", default=19)\n",
    "parser.add_argument(\"-e\", \"--evaluate\", dest=\"eval\", default=False)\n",
    "\"\"\"\n",
    "\n",
    "vocab_path = \"/home/ubuntu/ASAIL/jtnn_bo/jtnn/vocab.txt\"\n",
    "model_path = \"model.iter-0-1500\"\n",
    "save_dir = \"result/\"\n",
    "hidden_size = 200\n",
    "latent_size = 56\n",
    "depth = 3\n",
    "random_seed = 1\n",
    "training_num = 20000\n",
    "inducing_num = 500\n",
    "eval = \"False\"\n",
    "\n",
    "\n",
    "# We load the random seed\n",
    "np.random.seed(int(random_seed))\n",
    "\n",
    "# We load the data (y is minued!)\n",
    "kkk = int(training_num)\n",
    "M = int(inducing_num)\n",
    "X = np.loadtxt('./bo/latent_features2.txt')[:kkk]\n",
    "y = -np.loadtxt('./bo/targets2.txt')[:kkk]\n",
    "y = y.reshape((-1, 1))\n",
    "logP_values = np.loadtxt('./bo/logP_values2.txt')\n",
    "SA_scores = np.loadtxt('./bo/SA_scores2.txt')\n",
    "cycle_scores = np.loadtxt('./bo/cycle_scores2.txt')\n",
    "SA_scores_normalized = (np.array(SA_scores) - np.mean(SA_scores)) / np.std(SA_scores)\n",
    "logP_values_normalized = (np.array(logP_values) - np.mean(logP_values)) / np.std(logP_values)\n",
    "cycle_scores_normalized = (np.array(cycle_scores) - np.mean(cycle_scores)) / np.std(cycle_scores)\n",
    "#y = -logP_values\n",
    "#y = y[:kkk].reshape((-1, 1))\n",
    "#y = (np.array(y) - np.mean(y)) / np.std(y)\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "#device = \"cpu\"\n",
    "\n",
    "n = X.shape[0]\n",
    "\n",
    "permutation = np.random.choice(n, n, replace=False)\n",
    "\n",
    "X_train = X[permutation, :][0: np.int(np.round(0.9 * n)), :]\n",
    "X_test = X[permutation, :][np.int(np.round(0.9 * n)):, :]\n",
    "\n",
    "y_train = y[permutation][0: np.int(np.round(0.9 * n))]\n",
    "y_test = y[permutation][np.int(np.round(0.9 * n)):]\n",
    "\n",
    "y_train = y_train.transpose()\n",
    "y_test = y_test.transpose()\n",
    "\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float().to(device)\n",
    "y_train = torch.from_numpy(y_train).float().to(device)\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "\n",
    "\n",
    "# Train sparse Gaussian by gpytorch:\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel, InducingPointKernel\n",
    "from gpytorch.distributions import MultivariateNormal\n",
    "\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean()\n",
    "        self.base_covar_module = ScaleKernel(RBFKernel())\n",
    "        self.covar_module = InducingPointKernel(self.base_covar_module, inducing_points=train_x[:M, :], likelihood=likelihood)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    \n",
    "y_train, y_test = y_train.reshape(-1), y_test.reshape(-1)\n",
    "print(y_train.shape, y_test.shape)\n",
    "    \n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)\n",
    "model = GPRegressionModel(X_train, y_train, likelihood).to(device)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "#training_iterations = 200\n",
    "def train(training_iterations=20, optimizer=optimizer, X_train=X_train, y_train=y_train):\n",
    "    for i in range(training_iterations):\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(X_train)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, y_train)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "        #torch.cuda.empty_cache()\n",
    "if eval == \"False\":\n",
    "    with gpytorch.settings.use_toeplitz(True):\n",
    "        train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper function for BO\n",
    "from scipy.stats import norm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def update_posterior(X_new, Y_new, iter=10):\n",
    "\n",
    "    model.set_train_data(X_new, Y_new)\n",
    "    # optimize the GP hyperparameters using Adam with lr=0.005\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    train(training_iteration=iter, optimizer=optimizer)\n",
    "\n",
    "# Define Acquisition function\n",
    "# TODO: the original paper uses expected imporvement \n",
    "def lower_confidence_bound(x, kappa=2):\n",
    "    model.eval()\n",
    "    mu, variance = model(x).mean, model(x).variance\n",
    "    print(mu, variance)\n",
    "    sigma = variance.sqrt()\n",
    "    return mu - kappa * sigma\n",
    "\n",
    "def find_a_candidate(x_init, lb, ub):\n",
    "    # transform x to an unconstrained domain\n",
    "    constraint = constraints.interval(lb, ub)\n",
    "    #print(x_init)\n",
    "    unconstrained_x_init = transform_to(constraint).inv(x_init)\n",
    "    #print(unconstrained_x_init)\n",
    "    unconstrained_x = unconstrained_x_init.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # WARNING: this is a memory intensive optimizer\n",
    "    # TODO: Maybe try other gradient-based iterative methods\n",
    "    minimizer = optim.LBFGS([unconstrained_x], max_iter=10)\n",
    "\n",
    "    def closure():\n",
    "        minimizer.zero_grad()\n",
    "        x = transform_to(constraint)(unconstrained_x)\n",
    "        y = lower_confidence_bound(x)\n",
    "        #y = lower_confidence_bound(unconstrained_x)\n",
    "        #print(autograd.grad(y, unconstrained_x))\n",
    "        autograd.backward(unconstrained_x, autograd.grad(y, unconstrained_x))\n",
    "        return y\n",
    "\n",
    "    minimizer.step(closure)\n",
    "    # after finding a candidate in the unconstrained domain,\n",
    "    # convert it back to original domain.\n",
    "    x = transform_to(constraint)(unconstrained_x)\n",
    "    return x.detach()\n",
    "\n",
    "def next_x(lb, ub, num_candidates_each_x=5, num_x=60):\n",
    "    found_x=[]\n",
    "    x_init = model.train_inputs[0][-1:]\n",
    "    for j in range(num_x):\n",
    "        candidates = []\n",
    "        values = []\n",
    "        for i in range(num_candidates_each_x):\n",
    "            x = find_a_candidate(x_init, lb, ub)\n",
    "            y = lower_confidence_bound(x)\n",
    "            #print(\"next x:\", x, x.shape)\n",
    "            #print(\"next y:\", y, y.shape)\n",
    "            candidates.append(x)\n",
    "            values.append(y)\n",
    "            x_init = x.new_empty(1,56).uniform_(0, 1).mul(ub-lb).add_(lb).to(device)\n",
    "            #print(\"next\",x_init)\n",
    "        argmin = torch.min(torch.cat(values), dim=0)[1].item()\n",
    "        found_x.append(candidates[argmin])\n",
    "        x_init=found_x[-1]\n",
    "    return found_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0068], device='cuda:0', grad_fn=<ViewBackward>) tensor([1.2452], device='cuda:0', grad_fn=<ExpandBackward>)\n",
      "tensor([0.0068], device='cuda:0', grad_fn=<ViewBackward>) tensor([1.2452], device='cuda:0', grad_fn=<ExpandBackward>)\n",
      "tensor([0.0068], device='cuda:0', grad_fn=<ViewBackward>) tensor([1.2452], device='cuda:0', grad_fn=<ExpandBackward>)\n",
      "tensor([0.0068], device='cuda:0', grad_fn=<ViewBackward>) tensor([1.2452], device='cuda:0', grad_fn=<ExpandBackward>)\n",
      "tensor([0.0068], device='cuda:0', grad_fn=<ViewBackward>) tensor([1.2452], device='cuda:0', grad_fn=<ExpandBackward>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cb05dad17a10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mxmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mub\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-3a95a78579cf>\u001b[0m in \u001b[0;36mnext_x\u001b[0;34m(lb, ub, num_candidates_each_x, num_x)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_candidates_each_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_a_candidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlower_confidence_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;31m#print(\"next x:\", x, x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3a95a78579cf>\u001b[0m in \u001b[0;36mfind_a_candidate\u001b[0;34m(x_init, lb, ub)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mminimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;31m# after finding a candidate in the unconstrained domain,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# convert it back to original domain.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-3a95a78579cf>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#y = lower_confidence_bound(unconstrained_x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#print(autograd.grad(y, unconstrained_x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munconstrained_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munconstrained_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    147\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/functions/_inv_matmul.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(ctx, grad_output)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0;31m# Compute self^{-1} grad_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mleft_solves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInvMatmul\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmatrix_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneeds_input_grad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/functions/_inv_matmul.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, representation_tree, has_left, *args)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mleft_tensor\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0msolves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlazy_tsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msolves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/functions/_inv_matmul.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(lazy_tsr, rhs)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mpreconditioner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inv_matmul_preconditioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlazy_tsr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrhs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreconditioner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m_inv_matmul_preconditioner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0mfunction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mon\u001b[0m \u001b[0mx\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mperforms\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m^\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \"\"\"\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mbase_precond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preconditioner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbase_precond\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/lazy/added_diag_lazy_tensor.py\u001b[0m in \u001b[0;36m_preconditioner\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_woodbury_cache\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"self._q_cache\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_preconditioner_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_piv_chol_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpivoted_cholesky\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivoted_cholesky\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_piv_chol_self\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 warnings.warn(\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/utils/pivoted_cholesky.py\u001b[0m in \u001b[0;36mpivoted_cholesky\u001b[0;34m(matrix, max_iter, error_tol)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mL_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_diag_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_iters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi_m\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mrow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m   1690\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m         \u001b[0;31m# If we selected a single row and/or column (or did tensor indexing), we'll be retuning a tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, row_index, col_index, *batch_indices)\u001b[0m\n\u001b[1;32m     33\u001b[0m         results = [\n\u001b[1;32m     34\u001b[0m             \u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         ]\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSumLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/lazy/sum_lazy_tensor.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m         results = [\n\u001b[1;32m     34\u001b[0m             \u001b[0mlazy_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mlazy_tensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlazy_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         ]\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSumLazyTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/gpytorch/lazy/lazy_tensor.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, row_index, col_index, *batch_indices)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mrow_interp_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mrow_interp_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow_interp_indices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mrow_interp_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow_interp_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mcol_interp_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "lb = torch.min(X_train, dim = 0)[0]\n",
    "ub = torch.max(X_train, dim = 0)[0]\n",
    "xmin = next_x(lb,ub,5,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try gpytorch Bayesian optimization:\n",
    "# Define helper function for BO\n",
    "from scipy.stats import norm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def update_posterior(X_new, Y_new, iter=10):\n",
    "\n",
    "    model.set_train_data(X_new, Y_new)\n",
    "    # optimize the GP hyperparameters using Adam with lr=0.005\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "    train(training_iteration=iter, optimizer=optimizer)\n",
    "\n",
    "# Define Acquisition function\n",
    "# TODO: the original paper uses expected imporvement \n",
    "def lower_confidence_bound(x, kappa=2):\n",
    "    model.eval()\n",
    "    pred = model(x)\n",
    "    mu, variance = pred.mean, pred.variance\n",
    "    print(mu, variance)\n",
    "    sigma = variance.sqrt()\n",
    "    print(mu, kappa, sigma)\n",
    "    return mu - kappa * sigma\n",
    "\n",
    "def expected_improvement(x):\n",
    "    mu, variance = model(x, full_cov=False, noiseless=False)\n",
    "    x_sample = model.X\n",
    "    sigma = variance.sqrt()\n",
    "    mu_sample, v_sample = model(X_sample, full_cov=False, noiseless=False)\n",
    "    mu_sample_opt = torch.max(mu_sample)\n",
    "    imp = mu - mu_sample\n",
    "    z = imp / sigma\n",
    "    ei = F.relu(imp) + sigma * norm.pdf(z) - torch.abs(imp) * norm.cdf(z)\n",
    "    return ei\n",
    "        \n",
    "def find_a_candidate(x_init, lb, ub):\n",
    "\n",
    "    def acquisition_minimizer(x_init, iterations=100):\n",
    "        x = x_init.clone().requires_grad_(True)\n",
    "        minimizer = optim.Adam([x], lr=0.01)\n",
    "        for i in range(iterations):\n",
    "            y = lower_confidence_bound(x)\n",
    "            print(\"iteration - {}: lower_confidence_bound: {}\".format(i, y))\n",
    "            autograd.backward(x, autograd.grad(y, x), retain_graph=True)\n",
    "            minimizer.step()\n",
    "        return x\n",
    "    x = acquisition_minimizer(x_init)\n",
    " \n",
    "    print(\"minimizer finished\")\n",
    "    # after finding a candidate in the unconstrained domain,\n",
    "    # convert it back to original domain.\n",
    "    return x.detach()\n",
    "\n",
    "def next_x(lb, ub, num_candidates_each_x=5, num_x=60):\n",
    "    found_x=[]\n",
    "    x_init = model.train_inputs[0][-1:]\n",
    "    model.eval()\n",
    "    print(model(x_init).mean, model(x_init).variance)\n",
    "    for j in range(num_x):\n",
    "        candidates = []\n",
    "        values = []\n",
    "        for i in range(num_candidates_each_x):\n",
    "            print(\"start finding candidate\")\n",
    "            x = find_a_candidate(x_init, lb, ub)\n",
    "            print(x)\n",
    "            y = lower_confidencb\n",
    "            #print(\"next x:\", x, x.shape)\n",
    "            #print(\"next y:\", y, y.shape)\n",
    "            candidates.append(x)\n",
    "            values.append(y)\n",
    "            x_init = x.new_empty(1,56).uniform_(0, 1).mul(ub-lb).add_(lb).to(device)\n",
    "            #print(\"next\",x_init)\n",
    "        argmin = torch.min(torch.cat(values), dim=0)[1].item()\n",
    "        found_x.append(candidates[argmin])\n",
    "        x_init=found_x[-1]\n",
    "    return found_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lb = torch.min(X_train, dim = 0)[0]\n",
    "ub = torch.max(X_train, dim = 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xmin = next_x(lb,ub,5,60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(model(X_train[:1]).loc)\n",
    "print(model(X_train[:1]).mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
